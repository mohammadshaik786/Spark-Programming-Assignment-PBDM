{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdSng4W0E3Xy"
      },
      "source": [
        "**Installing the Spark Dependancies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZioYLN4EIgt",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget https://dlcdn.apache.org/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz\n",
        "!tar -xzf spark-3.3.0-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark==3.0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glH6hqJzFA4P"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.0-bin-hadoop3\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZkRDJa9Fd8M"
      },
      "source": [
        "Copy the Dataset into a local store.\n",
        "\n",
        "Download from: https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv\n",
        "\n",
        "(**In Colab:** the downloaded file is stored under \"/content\" folder)\n",
        "\n",
        "Dataset description can be found here: https://github.com/nytimes/covid-19-data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qadIjn_Z6Giz"
      },
      "outputs": [],
      "source": [
        "! wget https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOq5eADQhbyw"
      },
      "source": [
        "**Start your program**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6dZKZx1FAt1"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SQLContext, SparkSession"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WY_BKuuE94M"
      },
      "source": [
        "Create a Spark Session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oc1d5Df2FMJ5"
      },
      "outputs": [],
      "source": [
        "conf = SparkConf().set('spark.ui.port', '4050')\n",
        "sc = SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.master('local[*]').getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMLGiIF0Z2wf"
      },
      "source": [
        "Load dateset into a dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-_0QqoE48AL"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "# NOTE: Fix your dataset location in case you run locally on your machine\n",
        "data = spark.read.load('/content/us-counties.csv', format='csv', inferSchema=True, header=True)\n",
        "\n",
        "# Print schema\n",
        "data.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9fE-buM7rOl"
      },
      "outputs": [],
      "source": [
        "# The number of rows in the dataset\n",
        "data.count()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# See first 10 rows of the dataset\n",
        "data.show(10)"
      ],
      "metadata": {
        "id": "9ocUb8gGQEux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.schema"
      ],
      "metadata": {
        "id": "UZU2e7vjUDQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZS23AZqZ81a"
      },
      "source": [
        "**Task 0. Find the daily new cases across the entire US and plot**\n",
        "(*you DO not need to do this, this code is given for your assistance*)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pce2JYkU6dR1"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Aggregate by day, sum the cases for all counties for each day\n",
        "daily_cumulative = data.groupby('date').agg(F.sum('cases').alias('total_cases'))\n",
        "daily_cumulative = daily_cumulative.sort('date')\n",
        "daily_cumulative.show(10)\n",
        "\n",
        "# Convert Spark dataframe to a Panda dataframe to plot\n",
        "plot_data = daily_cumulative.toPandas()\n",
        "dates = plot_data['date']\n",
        "values = plot_data['total_cases']\n",
        "\n",
        "# Find daily news cases from cumulative cases\n",
        "daily_cases = [values[i+1] - values[i] for i in range(len(values)-1)]\n",
        "ddates = [dates[i+1] for i in range(len(values)-1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OlnFl4l0ec09"
      },
      "outputs": [],
      "source": [
        "# Plot daily cases against dates\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "plt.plot(ddates, daily_cases)\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Daily new cases')\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSO9J5J5HN0n"
      },
      "source": [
        "*Now, solve the following tasks.*\n",
        "\n",
        "Feel free to add more code blocks within each task to spereate the code for better clarity and understanding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHPGd1tDHrtV"
      },
      "source": [
        "**Task 1: Find the total number of new cases added in the entire US in the  month of March 2020.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5bJaPo-h2Ah7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaaFOC-xHy7e"
      },
      "outputs": [],
      "source": [
        "# Create temp table where in Pyspark we can use spark SQL\n",
        "data.registerTempTable(\"counties\") \n",
        "# Created CTE in which we will be returning date and sum of cases by grouping date column and hthen with condition having March 2020\n",
        "data_march = spark.sql('''with CTE as (\n",
        "  select date, sum(cases) as total from counties group by date having date like \"2020-03%\" order by date)\n",
        "  select sum(total) from CTE''')\n",
        "data_march.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCD7_J82HzqE"
      },
      "source": [
        "**Task 2: Calculate the total new cases added in three consecutive months of June, July, and August of 2020 in Jackson county, Missouri (fips code 29095).**\n",
        "\n",
        "Output will be like this:\n",
        "\n",
        "June 2020 `cases`\n",
        "\n",
        "July 2020 `cases`\n",
        "\n",
        "August 2020 `cases`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First CTE: extract3_months - where it returns records  with condition having three consecutive months of June, July, and August of 2020\n",
        "# Second CTE: sum3_months - returns data with 3 months having total new cases added\n",
        "# Finally by using case when statement, will be extracting month from the date and returning records.\n",
        "data_Jackson = spark.sql('''with extract3_months (\n",
        "  select date,fips,sum(cases) as total from counties group by fips,date having fips = 29095 AND (date like \"2020-06%\" OR date like \"2020-07%\" OR date like \"2020-08%\")),\n",
        "  sum3_months (\n",
        "  SELECT MONTH(date) AS Month, SUM(total) AS TotalSUM\n",
        "  FROM extract3_months\n",
        "  GROUP BY MONTH(date) order by Month)\n",
        "  select CASE\n",
        "    WHEN Month = 6 THEN \"June 2020 cases\"\n",
        "    WHEN Month = 7 THEN \"July 2020 cases\"\n",
        "    ELSE \"August 2020 cases\" \n",
        "  END AS MonthName,TotalSUM from sum3_months'''\n",
        "   )\n",
        "   \n",
        "data_Jackson.show()"
      ],
      "metadata": {
        "id": "IRF8g30Fbx7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtcLCf3HIPnx"
      },
      "source": [
        "**Task 3: Find the daily new cases per month per 1000 population in Missouri state (MO) since the beginning of the pandemic (assume MO's population is 6,154,913). [Plot the data]**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rounding sum of new cases per month per 1000 population and by using where condition we will be filtering state = 'Missouri' and Year(date) >= 2020 grouping the year and date\n",
        "data_Missouri = spark.sql(\"\"\" select round(sum(cases)*1000/6154913,2) as new_cases , Month(date) as Month, Year(date) \n",
        "as Year from counties where state = 'Missouri' and Year(date) >= 2020\n",
        "group by Month(date),Year(date) order by Year(date),Month(date)\"\"\")\n",
        "data_Missouri.show(30)"
      ],
      "metadata": {
        "id": "EYzI9i8ppYG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert Spark dataframe to a Panda dataframe to plot\n",
        "plot_data = data_Missouri.toPandas()\n",
        "Month = plot_data['Month']\n",
        "NewCases = plot_data['new_cases']\n",
        "\n",
        "# Find news cases from cumulative cases\n",
        "new_cases = [NewCases[i+1] - NewCases[i] for i in range(len(NewCases)-1)]\n",
        "month = [Month[i+1] for i in range(len(NewCases)-1)]\n"
      ],
      "metadata": {
        "id": "cghZdHAlp9Gp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot new cases against month\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "plt.plot(month, new_cases)\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('New Cases')\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AKmFC5b_pzip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neUh4Ez3IV8o"
      },
      "source": [
        "**Task 4:  On which date all 50 US states have at least 100 cases? At least one death?**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CTE- Firstly filtered records with cases>=100 AND deaths>=1and then by using CTE,filtering data by applying group by condition having all the states for particular date\n",
        "data_atleast = spark.sql('''with CTE (\n",
        "  SELECT * from counties where cases>=100 AND deaths>=1)\n",
        "  select date,count(distinct state) from CTE group by date having count(distinct state)>=50 order by date''')\n",
        "data_atleast.show()"
      ],
      "metadata": {
        "id": "k7JZhQULqsio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9i7LuBxIa2C"
      },
      "source": [
        "**Task 5: Which single day in the year 2020 and 2021 had the largest number of deaths in the entire US (if there are multiple such dates, choose the earliest one)?**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CTE - retuns data with all the dates having sum(deaths) and row_number with partition by YEAR(date)\n",
        "# Finally filtering 3 records in each year with rownumber=1 \n",
        "data_deaths = spark.sql('''with cte (select date,sum(deaths) as deaths, row_number() over(partition by YEAR(date) order by sum(deaths) desc) as rn from counties group by date)\n",
        "select date,DAY(date),deaths from cte where rn =1''')\n",
        "data_deaths.show()"
      ],
      "metadata": {
        "id": "Xy4zMwMSv4D1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWPsgNrbIihP"
      },
      "source": [
        "Your programming assignment ends here.\n",
        "Thank you."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fqD7sTuS0sIM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}